# IQN-MPC: Implicit Quantile Networks for Model Predictive Control

Distributional state transition modeling + uncertainty-aware planning for portfolio optimization.

## Algorithm Overview

### 1. IQN State Transition Model

**Goal:** Learn the full distribution P(s'|s,a), not just the mean.

**Architecture:**
```
Input: (state s, action a, quantile level Ï„ âˆˆ [0,1])
Output: Ï„-quantile of next state distribution

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  (s, a)     â”‚â”€â”€â”€â”€â–¶â”‚  Encoder (MLP)   â”‚â”€â”€â”€â”€â–¶ embedding e
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                              â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚     Ï„       â”‚â”€â”€â”€â”€â–¶â”‚ Cosine Embedding â”‚â”€â”€â”€â”€â–¶ Ï„_embed
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
                                              â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚     Decoder      â”‚â—€â”€â”€â”‚ e âŠ™ Ï„_e â”‚ (Hadamard product)
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
                         s'_Ï„ (predicted Ï„-quantile of next state)
```

**Cosine Embedding** (from Dabney et al. 2018):
```
Ï†(Ï„) = ReLU(W Â· [cos(0Â·Ï€Â·Ï„), cos(1Â·Ï€Â·Ï„), ..., cos((n-1)Â·Ï€Â·Ï„)]áµ€ + b)
```

**Training Loss** - Quantile Regression (Pinball Loss):
```
Ï_Ï„(u) = u Â· (Ï„ - ğŸ™{u < 0})

L = E_Ï„~U(0,1) [ Ï_Ï„(s'_true - s'_predicted) ]
```

This loss penalizes:
- Under-predictions (s'_pred < s'_true) by factor Ï„
- Over-predictions (s'_pred > s'_true) by factor (1-Ï„)

Result: The network learns to output the Ï„-quantile of the true distribution.

### 2. MPC Planning with CVaR

**Goal:** Find action sequence that maximizes risk-adjusted returns.

**CVaR (Conditional Value-at-Risk):**
```
CVaR_Î± = E[X | X â‰¤ VaR_Î±]  (average of worst Î± fraction of outcomes)
```

For Î±=0.2, we're optimizing for the worst 20% of scenarios â€” a risk-averse objective.

**Cross-Entropy Method (CEM) Planning:**
```
1. Initialize: Î¼ = 0, Ïƒ = 0.5 for action sequence distribution

2. For each CEM iteration:
   a. Sample N action sequences from N(Î¼, Ïƒ)
   
   b. For each sequence, simulate K trajectories:
      - At each step, sample Ï„ ~ U(0,1)
      - Predict next state: s' = IQN(s, a, Ï„)
      - Accumulate discounted rewards
   
   c. Compute CVaR for each sequence:
      - Sort trajectory returns
      - Average bottom Î± fraction
   
   d. Select top-E elite sequences (highest CVaR)
   
   e. Update: Î¼ = mean(elites), Ïƒ = std(elites)

3. Return first action of Î¼
```

### 3. Portfolio Application

**State:** [Ïƒ_AAPL, Ïƒ_BTC, Î¼_AAPL, Î¼_BTC] (volatilities + expected returns)

**Action:** [w_cash, w_AAPL, w_BTC] (portfolio weights as logits, softmax-normalized)

**Dynamics:** GARCH(1,1) process for each asset
```
ÏƒÂ²_t = Ï‰ + Î±Â·(r_{t-1} - Î¼)Â² + Î²Â·ÏƒÂ²_{t-1}
r_t = Î¼ + Ïƒ_t Â· Îµ_t,  Îµ_t ~ N(0,1)
```

**Why IQN helps:**
- GARCH has heavy tails â€” mean prediction misses tail risk
- IQN captures the full return distribution
- CVaR planning explicitly optimizes for worst-case scenarios

## Comparison with Markowitz

| Aspect | Markowitz | IQN-MPC |
|--------|-----------|---------|
| Model | None (uses realized Î¼, Ïƒ) | Learned P(s'\|s,a) |
| Objective | E[r] - Î»Â·Var[r] | CVaR_Î± (worst Î±% outcomes) |
| Planning | Single-step (myopic) | Multi-step lookahead |
| Uncertainty | Assumes Gaussian | Learns true distribution |

## Current Limitations

1. **CEM is sample-inefficient** â€” needs many samples for good CVaR estimates
2. **No JIT compilation** â€” slow evaluation (memory issues with JIT)
3. **Simple reward** â€” just expected return, could add transaction costs

## Files

- `iqn.py` â€” IQN network, quantile embedding, pinball loss, training
- `mpc.py` â€” CEM planner, trajectory sampling, CVaR computation
- `../scripts/eval_2asset_lean.py` â€” 2-asset portfolio evaluation

## References

- Dabney et al. (2018) "Implicit Quantile Networks for Distributional RL"
- Rockafellar & Uryasev (2000) "Optimization of CVaR"
- Chua et al. (2018) "PETS: Deep RL with Probabilistic Ensembles"
- Markowitz (1952) "Portfolio Selection"
